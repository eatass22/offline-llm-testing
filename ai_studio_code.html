<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Local RAG AI - Standalone</title>
    <style>
        :root {
            --bg-color: #1e1e1e;
            --sidebar-bg: #252526;
            --input-bg: #3c3c3c;
            --text-color: #ffffff;
            --accent-color: #007acc;
            --user-msg-bg: #0e639c;
            --ai-msg-bg: #333333;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            background-color: var(--bg-color);
            color: var(--text-color);
            margin: 0;
            display: flex;
            height: 100vh;
            overflow: hidden;
        }

        /* Sidebar for RAG/Knowledge */
        #sidebar {
            width: 300px;
            background-color: var(--sidebar-bg);
            padding: 20px;
            display: flex;
            flex-direction: column;
            border-right: 1px solid #444;
        }

        #knowledge-base {
            flex-grow: 1;
            background-color: var(--input-bg);
            color: var(--text-color);
            border: 1px solid #555;
            padding: 10px;
            resize: none;
            border-radius: 5px;
            font-size: 0.9rem;
            margin-top: 10px;
            margin-bottom: 10px;
        }

        /* Main Chat Area */
        #main {
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
        }

        #chat-history {
            flex-grow: 1;
            overflow-y: auto;
            padding: 20px;
            padding-bottom: 80px;
            display: flex;
            flex-direction: column;
            gap: 15px;
        }

        .message {
            max-width: 80%;
            padding: 10px 15px;
            border-radius: 10px;
            line-height: 1.5;
            animation: fadeIn 0.3s ease;
        }

        .user-message {
            align-self: flex-end;
            background-color: var(--user-msg-bg);
        }

        .ai-message {
            align-self: flex-start;
            background-color: var(--ai-msg-bg);
            border: 1px solid #444;
        }

        .context-indicator {
            font-size: 0.75rem;
            color: #aaa;
            margin-bottom: 5px;
            font-style: italic;
        }

        /* Input Area */
        #input-area {
            position: absolute;
            bottom: 0;
            left: 0;
            right: 0;
            background-color: var(--bg-color);
            padding: 20px;
            border-top: 1px solid #444;
            display: flex;
            gap: 10px;
        }

        #user-input {
            flex-grow: 1;
            padding: 12px;
            border-radius: 5px;
            border: 1px solid #555;
            background-color: var(--input-bg);
            color: white;
            font-size: 1rem;
        }

        button {
            padding: 10px 20px;
            background-color: var(--accent-color);
            color: white;
            border: none;
            border-radius: 5px;
            cursor: pointer;
            font-weight: bold;
        }

        button:disabled {
            background-color: #555;
            cursor: not-allowed;
        }

        button:hover:not(:disabled) {
            opacity: 0.9;
        }

        #status-bar {
            padding: 5px 20px;
            font-size: 0.8rem;
            color: #888;
            background-color: #111;
            text-align: right;
        }

        h2, h3 { margin-top: 0; }
        
        @keyframes fadeIn { from { opacity: 0; transform: translateY(10px); } to { opacity: 1; transform: translateY(0); } }
        
        /* Markdown basics */
        .ai-message code { background: #222; padding: 2px 5px; border-radius: 3px; font-family: monospace; }
        .ai-message pre { background: #222; padding: 10px; border-radius: 5px; overflow-x: auto; }
    </style>
</head>
<body>

    <!-- Sidebar: The Brain -->
    <div id="sidebar">
        <h3>ðŸ§  Knowledge Base</h3>
        <p style="font-size: 0.8rem; color: #aaa;">Paste documents, emails, or facts here. The AI will read this to answer your questions (RAG).</p>
        <textarea id="knowledge-base" placeholder="Paste text here to 'train' the AI instantly...
Example:
- Project X is due on Friday.
- The secret code is 42."></textarea>
        <div style="font-size: 0.8rem; color: #aaa;">
            Model: <span style="color: white;">Llama-3.2-3B-Instruct</span><br>
            Status: <span id="init-label">Initializing...</span>
        </div>
    </div>

    <!-- Main Chat Interface -->
    <div id="main">
        <div id="status-bar">Waiting for model to load...</div>
        <div id="chat-history">
            <!-- Messages go here -->
            <div class="message ai-message">
                Hello! I am downloading the model to your browser cache (this happens only once). 
                Once ready, you can chat with me offline and use the Knowledge Base sidebar to give me context.
            </div>
        </div>
        
        <form id="input-area">
            <input type="text" id="user-input" placeholder="Type your message..." disabled autocomplete="off">
            <button type="submit" id="send-btn" disabled>Send</button>
        </form>
    </div>

    <!-- Logic -->
    <script type="module">
        // Import WebLLM directly from CDN (No build step required)
        import { CreateMLCEngine } from "https://esm.run/@mlc-ai/web-llm";

        // Configuration
        // Llama 3.2 3B is highly optimized for browsers (approx 2GB download)
        const SELECTED_MODEL = "Llama-3.2-3B-Instruct-q4f16_1-MLC";

        let engine;
        let isModelLoaded = false;
        
        // DOM Elements
        const chatHistory = document.getElementById('chat-history');
        const userInput = document.getElementById('user-input');
        const sendBtn = document.getElementById('send-btn');
        const knowledgeBase = document.getElementById('knowledge-base');
        const statusLabel = document.getElementById('status-bar');
        const initLabel = document.getElementById('init-label');
        const form = document.getElementById('input-area');

        // Simple RAG Logic (Keyword/Overlap Retrieval)
        // Since we can't easily run a BERT embedding model AND an LLM in one file without crashing low-end devices,
        // we use a smart "Context Injection" strategy based on overlap.
        function getRelevantContext(query, fullText) {
            if (!fullText.trim()) return "";

            // 1. Split text into chunks (paragraphs)
            const chunks = fullText.split(/\n\s*\n/);
            
            // 2. Simple ranking: count word overlap between query and chunk
            const queryWords = query.toLowerCase().split(/\s+/).filter(w => w.length > 3);
            
            if (queryWords.length === 0) return fullText.substring(0, 2000); // Fallback to first 2k chars

            const scoredChunks = chunks.map(chunk => {
                const chunkLower = chunk.toLowerCase();
                let score = 0;
                queryWords.forEach(word => {
                    if (chunkLower.includes(word)) score++;
                });
                return { text: chunk, score: score };
            });

            // 3. Sort by score and take top 3 chunks
            scoredChunks.sort((a, b) => b.score - a.score);
            const topChunks = scoredChunks.filter(c => c.score > 0).slice(0, 3);

            if (topChunks.length === 0) return ""; // No relevant context found

            return topChunks.map(c => c.text).join("\n---\n");
        }

        // Initialize Engine
        async function init() {
            try {
                // Callback to update progress bar
                const initProgressCallback = (report) => {
                    console.log(report.text);
                    statusLabel.innerText = report.text;
                    initLabel.innerText = "Loading... " + (report.progress * 100).toFixed(0) + "%";
                };

                engine = await CreateMLCEngine(
                    SELECTED_MODEL,
                    { initProgressCallback: initProgressCallback }
                );

                isModelLoaded = true;
                statusLabel.innerText = "Model Ready (Offline Capable)";
                initLabel.innerText = "Ready";
                userInput.disabled = false;
                sendBtn.disabled = false;
                userInput.focus();
                
                appendMessage("ai", "I'm ready! Paste text in the Knowledge Base to give me context, or just start chatting.");

            } catch (error) {
                console.error(error);
                statusLabel.innerText = "Error: WebGPU not supported or download failed.";
                appendMessage("ai", "Error: Your browser might not support WebGPU. Please use Chrome, Edge, or Brave on a desktop/laptop.");
            }
        }

        // UI Logic: Append Message
        function appendMessage(role, text, contextUsed = false) {
            const div = document.createElement('div');
            div.className = `message ${role}-message`;
            
            let contentHtml = text;
            
            // Simple markdown formatting for code blocks
            contentHtml = contentHtml.replace(/```([\s\S]*?)```/g, '<pre><code>$1</code></pre>');
            
            if (contextUsed) {
                const indicator = document.createElement('div');
                indicator.className = 'context-indicator';
                indicator.innerText = "ðŸ’¡ Used information from Knowledge Base";
                div.appendChild(indicator);
            }

            const contentDiv = document.createElement('div');
            contentDiv.innerHTML = contentHtml.replace(/\n/g, '<br>');
            div.appendChild(contentDiv);

            chatHistory.appendChild(div);
            chatHistory.scrollTop = chatHistory.scrollHeight;
        }

        // Handle Send
        form.addEventListener('submit', async (e) => {
            e.preventDefault();
            const text = userInput.value.trim();
            if (!text || !isModelLoaded) return;

            // 1. Show User Message
            appendMessage('user', text);
            userInput.value = '';
            userInput.disabled = true;
            sendBtn.disabled = true;

            // 2. Perform RAG (Retrieval)
            const kbText = knowledgeBase.value;
            const relevantContext = getRelevantContext(text, kbText);
            
            // 3. Construct Prompt with Context
            let systemPrompt = "You are a helpful AI assistant. Answer the user's question accurately.";
            if (relevantContext) {
                systemPrompt += `\n\nCONTEXT FROM KNOWLEDGE BASE:\n${relevantContext}\n\nINSTRUCTION: Use the context above to answer the user's question. If the answer isn't in the context, use your general knowledge but mention you are unsure.`;
            }

            const messages = [
                { role: "system", content: systemPrompt },
                // We let the engine manage history, but for a single file simple implementation,
                // we'll pass the immediate turn. For full history, we'd maintain an array here.
                { role: "user", content: text }
            ];

            // 4. Generate Response
            try {
                const chunks = await engine.chat.completions.create({
                    messages: messages,
                    temperature: 0.6,
                    stream: true, // Stream the response
                });

                let aiResponseText = "";
                const responseDiv = document.createElement('div');
                responseDiv.className = 'message ai-message';
                
                if (relevantContext) {
                    const indicator = document.createElement('div');
                    indicator.className = 'context-indicator';
                    indicator.innerText = "ðŸ’¡ Analyzing Knowledge Base...";
                    responseDiv.appendChild(indicator);
                }

                const textSpan = document.createElement('div');
                responseDiv.appendChild(textSpan);
                chatHistory.appendChild(responseDiv);

                for await (const chunk of chunks) {
                    const delta = chunk.choices[0]?.delta?.content || "";
                    aiResponseText += delta;
                    textSpan.innerHTML = aiResponseText.replace(/\n/g, '<br>'); // Simple render
                    chatHistory.scrollTop = chatHistory.scrollHeight;
                }
                
                // Keep engine history in sync (WebLLM does this internally usually, but precise control helps)
                // In this simple demo, we rely on WebLLM's internal state management.

            } catch (err) {
                appendMessage('ai', "Error generating response: " + err.message);
            }

            userInput.disabled = false;
            sendBtn.disabled = false;
            userInput.focus();
        });

        // Start
        init();

    </script>
</body>
</html>